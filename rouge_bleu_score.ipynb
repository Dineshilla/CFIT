{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "import warnings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Function to calculate BLEU score using NLTK\n",
    "def calculate_bleu(references, candidates):\n",
    "    bleu_scores = []\n",
    "    for ref, cand in zip(references, candidates):\n",
    "        bleu_score = nltk.translate.bleu_score.sentence_bleu([nltk.word_tokenize(ref)], nltk.word_tokenize(cand))\n",
    "        bleu_scores.append(bleu_score)\n",
    "    return sum(bleu_scores) / len(bleu_scores)  # Return average BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "def nearest_house_distance(N, house_positions, house_widths, M, people_positions):\n",
    "    # Create a list of ranges for each house\n",
    "    house_ranges = [\n",
    "        (pos, pos + width - 1)\n",
    "        for pos, width in zip(house_positions, house_widths)\n",
    "    ]\n",
    "    \n",
    "    total_distance = 0\n",
    "    \n",
    "    # For each person, calculate the minimum distance to the nearest house\n",
    "    for person in people_positions:\n",
    "        min_distance = float('inf')\n",
    "        \n",
    "        for house_start, house_end in house_ranges:\n",
    "            if house_start <= person <= house_end:\n",
    "                # Person is already inside the house\n",
    "                min_distance = 0\n",
    "                break\n",
    "            else:\n",
    "                # Calculate distance to the nearest point of the house\n",
    "                distance = min(abs(person - house_start), abs(person - house_end))\n",
    "                min_distance = min(min_distance, distance)\n",
    "        \n",
    "        total_distance += min_distance\n",
    "    \n",
    "    return total_distance\n",
    "\n",
    "# Example input\n",
    "N = 2\n",
    "house_positions = [15,43]\n",
    "house_widths = [15,2]\n",
    "M = 3\n",
    "people_positions = [30,4,44]\n",
    "\n",
    "# Output the result\n",
    "result = nearest_house_distance(N, house_positions, house_widths, M, people_positions)\n",
    "print(result)  # Output: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/co/major_project\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# def cosine_similarity(text1, text2):\n",
    "#     \"\"\"\n",
    "#     Calculates the cosine similarity between two arrays of strings.\n",
    "\n",
    "#     Args:\n",
    "#         text1 (list): A list of strings.\n",
    "#         text2 (list): A list of strings.\n",
    "\n",
    "#     Returns:\n",
    "#         float: The cosine similarity between the two input arrays.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Preprocessing\n",
    "#     stop_words = stopwords.words('english')\n",
    "\n",
    "#     def preprocess(text):\n",
    "#         tokens = word_tokenize(text.lower())\n",
    "#         return [w for w in tokens if w not in stop_words]\n",
    "\n",
    "#     vectorizer = np.vectorize(preprocess)  # Efficiently preprocess lists of strings\n",
    "#     text1_processed = vectorizer(text1)\n",
    "#     text2_processed = vectorizer(text2)\n",
    "\n",
    "#     # Create common vocabulary\n",
    "#     all_words = np.unique(np.concatenate([text1_processed, text2_processed]))\n",
    "\n",
    "#     # Create vectors\n",
    "#     def text_to_vector(text):\n",
    "#         vector = np.zeros(len(all_words))\n",
    "#         for w in text:\n",
    "#             if w in all_words:\n",
    "#                 vector[np.where(all_words == w)[0][0]] = 1\n",
    "#         return vector\n",
    "\n",
    "#     vector1 = text_to_vector(text1_processed)\n",
    "#     vector2 = text_to_vector(text2_processed)\n",
    "\n",
    "#     # Calculate cosine similarity\n",
    "#     cosine = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "#     return cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def cosine_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two arrays of strings. Handles cases\n",
    "    where the input consists of lists of lists of words.\n",
    "\n",
    "    Args:\n",
    "        text1 (list): A list of strings or a list of lists of words.\n",
    "        text2 (list): A list of strings or a list of lists of words.\n",
    "\n",
    "    Returns:\n",
    "        float: The cosine similarity between the two input arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    def preprocess(text):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        return [w for w in tokens if w not in stop_words]\n",
    "\n",
    "    # Handle both strings and lists of words directly\n",
    "    if isinstance(text1[0], str):\n",
    "        text1_processed = [preprocess(text) for text in text1]\n",
    "    else:\n",
    "        text1_processed = text1\n",
    "    if isinstance(text2[0], str):\n",
    "        text2_processed = [preprocess(text) for text in text2]\n",
    "    else:\n",
    "        text2_processed = text2\n",
    "\n",
    "    # Create common vocabulary\n",
    "    all_words = set()\n",
    "    for text in text1_processed + text2_processed:\n",
    "        all_words.update(text)\n",
    "    all_words = list(all_words)  # Convert back to a list for indexing\n",
    "\n",
    "    # Create vectors\n",
    "    def text_to_vector(text):\n",
    "        vector = np.zeros(len(all_words))\n",
    "        for w in text:\n",
    "            if w in all_words:\n",
    "                vector[all_words.index(w)] = 1\n",
    "        return vector\n",
    "\n",
    "    vector1 = text_to_vector(text1_processed)\n",
    "    vector2 = text_to_vector(text2_processed)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "    return cosine\n",
    "\n",
    "\n",
    "# Example Usage (assuming your data is already preprocessed as lists of lists)\n",
    "text1 = [[\"I\", \"love\", \"horror\", \"movies\"], [\"The\", \"supernatural\", \"scares\"]]\n",
    "text2 = [[\"Lights\", \"out\", \"is\", \"horror\", \"movie\"], [\"Horror\", \"movies\", \"are\", \"favorite\"]]\n",
    "\n",
    "similarity = cosine_similarity(text1, text2)\n",
    "print(similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge(references, candidates):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    for ref, cand in zip(references, candidates):\n",
    "        score = scorer.score(ref, cand)\n",
    "        scores['rouge1'].append(score['rouge1'].fmeasure)\n",
    "        scores['rouge2'].append(score['rouge2'].fmeasure)\n",
    "        scores['rougeL'].append(score['rougeL'].fmeasure)\n",
    "\n",
    "    # Compute average scores\n",
    "    avg_scores = {key: sum(val) / len(val) for key, val in scores.items()}\n",
    "    return avg_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.012533508243433479\n",
      "ROUGE score: {'rouge1': 0.2831193623897731, 'rouge2': 0.07841638506130696, 'rougeL': 0.2386720187988354}\n",
      "Cosine Similarity nan\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "data = pd.read_csv('/Users/co/major_project/test_tinyllama-original.csv')\n",
    "\n",
    "# Compute BLEU and ROUGE scores\n",
    "bleu_score = calculate_bleu(data['Answer'].to_list(), data['Predicted'].to_list())\n",
    "rouge_score = calculate_rouge(data['Answer'].to_list(), data['Predicted'].to_list())\n",
    "similarity = cosine_similarity(data['Answer'].to_list(), data['Predicted'].to_list())\n",
    "\n",
    "print('BLEU score:', bleu_score)\n",
    "print('ROUGE score:', rouge_score)\n",
    "print(\"Cosine Similarity\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "\n",
    "# Function to calculate BLEU score using NLTK\n",
    "def calculate_bleu(references, candidates):\n",
    "    bleu_scores = []\n",
    "    for ref_group, cand in zip(references, candidates):\n",
    "        # Tokenize each reference in the group\n",
    "        tokenized_refs = [nltk.word_tokenize(ref) for ref in ref_group]\n",
    "        bleu_score = nltk.translate.bleu_score.sentence_bleu(tokenized_refs, nltk.word_tokenize(cand))\n",
    "        bleu_scores.append(bleu_score)\n",
    "    return sum(bleu_scores) / len(bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate ROUGE score\n",
    "def calculate_rouge(references, candidates):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    for ref, cand in zip(references, candidates):\n",
    "        # Use the first reference for ROUGE calculation (or could modify to use all)\n",
    "        score = scorer.score(ref[0], cand)\n",
    "        scores['rouge1'].append(score['rouge1'].fmeasure)\n",
    "        scores['rouge2'].append(score['rouge2'].fmeasure)\n",
    "        scores['rougeL'].append(score['rougeL'].fmeasure)\n",
    "\n",
    "    # Compute average scores\n",
    "    avg_scores = {key: sum(val) / len(val) for key, val in scores.items()}\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the CSV file\n",
    "# data = pd.read_csv('test_predicted (1).csv')\n",
    "\n",
    "\n",
    "# # Check if 'answer1' column exists\n",
    "# if 'answer1' in data.columns:\n",
    "#     # Keep only rows where 'answer1' is a non-empty string\n",
    "#     df = data[data['answer1'].apply(lambda x: isinstance(x, str) and x.strip() != '')]\n",
    "#     # print(\"Rows with empty, NaN, or non-string values in 'answer1' have been dropped.\")\n",
    "# else:\n",
    "#     # print(\"Column 'answer1' does not exist in the DataFrame.\")\n",
    "#     pass\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv('test_predicted (1).csv')\n",
    "\n",
    "# Filter out rows where 'answer1' is not a string or is empty/Nan\n",
    "data = data[data['answer1'].apply(lambda x: isinstance(x, str) and x.strip() != '')]\n",
    "\n",
    "# Assuming 'answer1' contains multiple references separated by '||'\n",
    "data['answer1'] = data['answer1'].apply(lambda x: x.split('||') if isinstance(x, str) else [])\n",
    "\n",
    "# Prepare data for evaluation\n",
    "predictions = data['generated_answer'].tolist()\n",
    "references = data['answer1'].tolist()  # Now 'answer1' should be a list of references\n",
    "\n",
    "# Now you can compute BLEU and ROUGE scores as before\n",
    "\n",
    "\n",
    "\n",
    "# Prepare data for evaluation\n",
    "predictions = data['generated_answer'].tolist()\n",
    "references = [row['answer1'] for _, row in data.iterrows()]  # Assuming '||' is used to separate references\n",
    "\n",
    "print(\"predictions\",predictions)\n",
    "print(\"references\",references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BLEU and ROUGE scores\n",
    "bleu_score = calculate_bleu(references, predictions)\n",
    "rouge_score = calculate_rouge(references, predictions)\n",
    "\n",
    "print('BLEU score:', bleu_score)\n",
    "print('ROUGE score:', rouge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
